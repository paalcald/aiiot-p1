{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "Importamos las librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from scipy import sparse\n",
        "from surprise import Dataset, Reader, accuracy, SVD, NMF\n",
        "from surprise.model_selection import train_test_split\n",
        "import surprise.prediction_algorithms.knns as knns\n",
        "import surprise.prediction_algorithms.matrix_factorization\n",
        "from sklearn.metrics import ndcg_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importamos los datos del dataset de manera manual para disponer de movies.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We load all the variables we care about\n",
        "ratings= pd.read_csv(\"ml-100k/ratings.csv\")\n",
        "movies = pd.read_csv('ml-100k/movies.csv')\n",
        "tags = pd.read_csv('ml-100k/tags.csv')\n",
        "links = pd.read_csv('ml-100k/links.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos la forma de los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ratings['rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que tenemos:\n",
        "- userId: ID del usuario que dejó la calificación\n",
        "- movieId: ID de la pelicula calificada\n",
        "- rating: Calificación de 1 a 5 \n",
        "- timestamp: Unidad de tiempo en la que se dejó la clasificación\n",
        "\n",
        "Convertimos los datos para la libreria Surprise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reader = Reader(rating_scale=(1,5))\n",
        "# Train/Test split using pandas alone\n",
        "#train = ratings.sample(frac=0.75, random_state=1234)\n",
        "#test = ratings.drop(train.index)\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Separamos los conjuntos de train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainset, testset = train_test_split(data, test_size=0.25, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Realizamos las predicciones con diferentes algoritmos\n",
        "\n",
        "Obtenemos predicciones con KNNBasic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn = knns.KNNBasic(sim_options={\"name\": \"pearson\"})\n",
        "knn.fit(trainset)\n",
        "knn_pred = knn.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con KNNBasic basado en productos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_prod = knns.KNNBasic(sim_options={\"name\": \"pearson\", 'user_based': False})\n",
        "knn_prod.fit(trainset)\n",
        "knn_prod_pred = knn_prod.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "svd = SVD()\n",
        "svd.fit(trainset)\n",
        "svd_predictions = svd.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nmf = NMF()\n",
        "nmf.fit(trainset)\n",
        "nmf_predictions = svd.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions = {\"KNNSBasic User Based\": knn_pred, \"KNNSBasic Product Based\": knn_prod_pred, \"SVD\": svd_predictions, \"NMF\": nmf_predictions}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Funcion auxiliar para obtener los resultados de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_titles(prediction, n, movies):\n",
        "    return [movies.loc[movies['movieId'] == recomended_id, 'title'].item() for recomended_id in [item.iid for item in prediction[0:n]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Obtenemos las predicciones para cada algoritmo\n",
        "KNNSBasic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_titles(knn_pred, 30, movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KNNSBasic products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_titles(knn_prod_pred, 10, movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_titles(svd_predictions, 30, movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_titles(nmf_predictions, 10, movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Función auxiliar para generar el diccionario de usuarios y predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def groupby_id(d, p):\n",
        "    if p.uid in d:\n",
        "        d[p.uid].append(p)\n",
        "        return d\n",
        "    else:\n",
        "        d[p.uid] = [p]\n",
        "        return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos nuestra lista de recomendaciones y la ordenamos por positivas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_recomendation_list(predictions):\n",
        "    recomendation_list = reduce(groupby_id, predictions, {})\n",
        "    for recomendations in recomendation_list.values():\n",
        "        recomendations.sort(key=lambda x: x.est, reverse=True)\n",
        "    return recomendation_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def recallAtK(rec_list, k = 10):\n",
        "    out = {}\n",
        "    for user, user_preds in rec_list.items():\n",
        "        relevant_k = len(list(filter(lambda p: p.r_ui >= 4 and p.est >=4, user_preds[:k])))\n",
        "        n_rel = len(list(filter(lambda p: p.r_ui >= 4, user_preds)))\n",
        "        if len(user_preds) >= k:\n",
        "            out[user] = relevant_k / n_rel if n_rel != 0 else 0\n",
        "    return out\n",
        "\n",
        "def precisionAtK(rec_list, k = 10):\n",
        "    out = {}\n",
        "    for user, user_preds in rec_list.items():\n",
        "        relevant_k = len(list(filter(lambda p: p.r_ui >= 4 and p.est >=4, user_preds[:k])))\n",
        "        n_rec_k = len(list(filter(lambda p: p.est >= 4, user_preds[:k])))\n",
        "        if len(user_preds) >= k:\n",
        "            out[user] = relevant_k / n_rec_k if n_rec_k != 0 else 0\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#No entiendo NDCG esto está sacado de github issues para surprise\n",
        "def get_ndcg(surprise_predictions, k_highest_scores=None):\n",
        "    uids = [int(p.uid) for p in surprise_predictions ]\n",
        "    iids = [int(p.iid) for p in surprise_predictions ]\n",
        "    r_uis = [p.r_ui for p in surprise_predictions ]\n",
        "    ests = [p.est for p in surprise_predictions ]\n",
        "    \n",
        "    assert(len(uids) == len(iids) == len(r_uis) == len(ests) )    \n",
        "    \n",
        "    sparse_preds = sparse.coo_matrix( (ests, (uids , iids )) )\n",
        "    sparse_vals = sparse.coo_matrix( (r_uis, (uids , iids )) )\n",
        "    \n",
        "    dense_preds = sparse_preds.toarray()\n",
        "    dense_vals = sparse_vals.toarray()\n",
        "    \n",
        "    return ndcg_score(y_true= dense_vals , y_score= dense_preds, k=k_highest_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculamos accuracy, Recall y precision @ 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "metricas_evaluacion = {\"Accuracy\": [], \"Recall@k\": [], \"Precision@k\": [], \"NDGC\": []}\n",
        "for pred in predictions.values():\n",
        "    metricas_evaluacion[\"Accuracy\"].append(accuracy.rmse(pred))\n",
        "    recs = get_recomendation_list(pred)\n",
        "    recalls = recallAtK(recs)\n",
        "    avg_recall = sum(recalls.values()) / len(recalls)\n",
        "    metricas_evaluacion[\"Recall@k\"].append(avg_recall)\n",
        "    precision = precisionAtK(recs)\n",
        "    avg_precision = sum(precision.values()) / len(recalls)\n",
        "    metricas_evaluacion[\"Precision@k\"].append(avg_precision)\n",
        "    ndgc = get_ndcg(pred)\n",
        "    metricas_evaluacion[\"NDGC\"].append(ndgc)\n",
        "metricas = pd.DataFrame(data=metricas_evaluacion, index=predictions.keys())\n",
        "metricas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusion\n",
        "\n",
        "Parece ser que en todas las métricas el que mejor funciona para este dataset es KNNSBasic basado en usuarios.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/home/paalcald/.virtualenvs/dlai/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}